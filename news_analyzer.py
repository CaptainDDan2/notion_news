"""
ë‰´ìŠ¤ ë¶„ì„ ë° ìš”ì•½ ëª¨ë“ˆ
AIë¥¼ í™œìš©í•œ ê¸°ì‚¬ ìš”ì•½ê³¼ ìš°ì„ ìˆœìœ„ ê³„ì‚°
"""

import os
import re
import logging
from typing import Dict, List
from datetime import datetime, timedelta
import openai
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class NewsAnalyzer:
    def __init__(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        # OpenAI API í‚¤ ì„¤ì •
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
        
        # ì¤‘ìš”ë„ í‰ê°€ í‚¤ì›Œë“œ
        self.high_priority_keywords = [
            'breakthrough', 'revolutionary', 'first time', 'ìµœì´ˆ', 'í˜ì‹ ',
            'merger', 'acquisition', 'ì¸ìˆ˜', 'í•©ë³‘', 'partnership', 'íŒŒíŠ¸ë„ˆì‹­',
            'IPO', 'ìƒì¥', 'earnings', 'ì‹¤ì ', 'patent', 'íŠ¹í—ˆ',
            'TSMC', 'Samsung', 'Intel', 'NVIDIA', 'AMD', 'Qualcomm',
            'Apple', 'Google', 'ì‚¼ì„±', 'í•˜ì´ë‹‰ìŠ¤'
        ]
        
        self.medium_priority_keywords = [
            'development', 'ê°œë°œ', 'launch', 'ì¶œì‹œ', 'announcement', 'ë°œí‘œ',
            'upgrade', 'ì—…ê·¸ë ˆì´ë“œ', 'expansion', 'í™•ì¥', 'investment', 'íˆ¬ì'
        ]
        
        # ë°˜ë„ì²´ ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ì ìš©) - ë°˜ë„ì²´ ê³µì •, ì†Œì, TSMC, ì‚¼ì„±, í•˜ì´ë‹‰ìŠ¤ ìµœìš°ì„ 
        self.tech_keywords = {
            # ìµœìƒìœ„ í•µì‹¬ ë°˜ë„ì²´ ê¸°ì—… ë° ê³µì • (ìµœìš°ì„ )
            'TSMC': 5.0, 'tsmc': 5.0,
            'ì‚¼ì„±': 5.0, 'Samsung': 5.0, 'samsung': 4.8,
            'í•˜ì´ë‹‰ìŠ¤': 5.0, 'SK Hynix': 5.0, 'Hynix': 4.8,
            
            # ìµœìƒìœ„ ë°˜ë„ì²´ ê³µì • ë° ì†Œì
            'ë°˜ë„ì²´': 5.0, 'semiconductor': 5.0,
            'ê³µì •': 4.5, 'process': 4.3, 'manufacturing': 4.3,
            'ì†Œì': 4.8, 'device': 4.5, 'chip': 4.2,
            
            # ê³ ê¸‰ ê³µì • ë…¸ë“œ (2nm, 3nm ìµœìš°ì„ )
            '2nm': 5.0, '1nm': 5.0,
            '3nm': 4.8, '5nm': 4.2, '7nm': 3.8,
            'GAA': 4.2, 'FinFET': 3.8,
            
            # ë©”ëª¨ë¦¬ ê¸°ìˆ 
            'HBM': 4.5, 'HBM4': 5.0, 'HBM3': 4.5,
            'ë©”ëª¨ë¦¬': 4.0, 'memory': 4.0,
            'DRAM': 4.0, 'NAND': 4.0, 'Flash': 3.8,
            
            # íŒŒìš´ë“œë¦¬
            'íŒŒìš´ë“œë¦¬': 4.5, 'foundry': 4.5,
            
            # AI ë° ê³ ê¸‰ ê¸°ìˆ 
            'AI': 3.8, 'ì¸ê³µì§€ëŠ¥': 3.8, 'ë¨¸ì‹ ëŸ¬ë‹': 3.2, 'machine learning': 3.2,
            'quantum': 3.8, 'ì–‘ì': 3.8, 'neuromorphic': 3.5,
            'edge computing': 3.0, 'ì—£ì§€ ì»´í“¨íŒ…': 3.0,
            'autonomous': 3.2, 'ììœ¨ì£¼í–‰': 3.2, 'IoT': 2.8, 'blockchain': 2.5
        }

    def _is_english_text(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ì˜ì–´ì¸ì§€ ê°ì§€ (í•œê¸€ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ ì˜ì–´)"""
        if not text:
            return False
        korean_char_count = sum(1 for c in text if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3)
        total_chars = len(text)
        korean_ratio = korean_char_count / total_chars if total_chars > 0 else 0
        return korean_ratio < 0.2  # í•œê¸€ì´ 20% ë¯¸ë§Œì´ë©´ ì˜ì–´

    def _translate_text(self, text: str, is_title: bool = False) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ í•œê¸€ë¡œ ë²ˆì—­"""
        if not self._is_english_text(text):
            return text
            
        try:
            if self.openai_api_key and len(text) > 10:
                max_tokens = 100 if is_title else 300
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {
                            "role": "system",
                            "content": "You are a professional translator. Translate the given English text to natural Korean. Provide only the translation, nothing else."
                        },
                        {
                            "role": "user",
                            "content": text
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=0.3
                )
                translated = response.choices[0].message.content.strip()
                return translated if translated else text
        except Exception as e:
            logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)}")
        
        return text

    def summarize_article(self, content: str, max_length: int = 400) -> str:
        """ê¸°ì‚¬ ìƒì„¸ ìš”ì•½ ìƒì„± - êµ¬ì²´ì ì´ê³  êµ¬ì¡°í™”ëœ ìš”ì•½"""
        try:
            if self.openai_api_key and len(content) > 100:
                return self._summarize_with_openai(content, max_length)
            else:
                return self._enhanced_simple_summarize(content, max_length)
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._enhanced_simple_summarize(content, max_length)

    def _summarize_with_openai(self, content: str, max_length: int) -> str:
        """OpenAI APIë¥¼ ì‚¬ìš©í•œ êµ¬ì²´ì ì¸ ìš”ì•½"""
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì‚°ì—… ì „ë¬¸ê°€ì´ì ì·¨ì—… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì·¨ì—… ì¤€ë¹„ìƒë“¤ì´ ì´ë ¥ì„œ, ë©´ì ‘, ì—ì„¸ì´ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‹¤ìš©ì ì¸ í•œêµ­ì–´ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”:

ğŸ’¼ **ì‚°ì—… ë™í–¥ & ê¸°ìˆ  ì´í•´**
ì´ ë‰´ìŠ¤ê°€ ë°˜ë„ì²´ ì‚°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ í•µì‹¬ ê¸°ìˆ ì„ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ì·¨ì—… ì¤€ë¹„ìƒì´ "ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ì´í•´í•˜ê³  ìˆë‹¤"ê³  ì–´í•„í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.

ğŸ­ **ì£¼ìš” ê¸°ì—… ë¶„ì„ & ì·¨ì—… ì‹œì¥**
ê´€ë ¨ ê¸°ì—…ë“¤ì˜ ì‚¬ì—… ì „ëµê³¼ ì‹œì¥ í¬ì§€ì…˜ì„ ì„¤ëª…í•˜ê³ , í•´ë‹¹ ê¸°ì—…ë“¤ì˜ ì±„ìš© ë™í–¥ì´ë‚˜ í•„ìš” ì—­ëŸ‰ê³¼ ì—°ê²°í•´ì£¼ì„¸ìš”. ì–´ë–¤ ì§ë¬´ì— ë„ì›€ì´ ë ì§€ë„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

ğŸ“ˆ **êµ¬ì²´ì  ì„±ê³¼ ì§€í‘œ**
ë©´ì ‘ì—ì„œ ì–¸ê¸‰í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ìˆ˜ì¹˜ë“¤(ì„±ëŠ¥ ê°œì„ ë¥ , íˆ¬ì ê·œëª¨, ì‹œì¥ ê·œëª¨ ë“±)ì„ ì •ë¦¬í•˜ê³ , ì´ ìˆ˜ì¹˜ë“¤ì´ ì—…ê³„ì—ì„œ ê°–ëŠ” ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ğŸ¯ **ì»¤ë¦¬ì–´ ì—°ê´€ì„±**
ì´ ê¸°ìˆ /ì‚°ì—… ë³€í™”ê°€ í–¥í›„ 5-10ë…„ê°„ ì–´ë–¤ ìƒˆë¡œìš´ ì§ì—…ì´ë‚˜ ì—­ëŸ‰ ìˆ˜ìš”ë¥¼ ë§Œë“¤ì–´ë‚¼ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”. ì·¨ì—… ì¤€ë¹„ìƒì´ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì¤€ë¹„í•˜ë©´ ì¢‹ì„ì§€ ì œì‹œí•´ì£¼ì„¸ìš”.

ğŸ’¡ **ë©´ì ‘ í™œìš© í¬ì¸íŠ¸**
ì´ ë‚´ìš©ì„ ë©´ì ‘ì—ì„œ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”. "ì—…ê³„ ë™í–¥ì— ëŒ€í•œ ì´í•´ë„"ë‚˜ "ë¯¸ë˜ ë¹„ì „"ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ë‹µë³€ ì†Œì¬ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.

ê° ì„¹ì…˜ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ê³ , ì·¨ì—… ì¤€ë¹„ìƒ ê´€ì ì—ì„œ ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
                    },
                    {
                        "role": "user", 
                        "content": f"ë‹¤ìŒ ë°˜ë„ì²´ ê¸°ì‚¬ë¥¼ ìœ„ í˜•ì‹ìœ¼ë¡œ ì·¨ì—… ì¤€ë¹„ìƒ ê´€ì ì—ì„œ ë¶„ì„í•˜ì—¬ ì‹¤ìš©ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš” (ì´ {max_length*3}ì ì´ë‚´, ë©´ì ‘/ì´ë ¥ì„œ í™œìš© ê°€ëŠ¥í•˜ë„ë¡):\n\n{content[:4000]}"
                    }
                ],
                max_tokens=int(max_length*2),
                temperature=0.2
            )
            summary = response.choices[0].message.content.strip()
            
            # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            if len(summary) > max_length * 3:
                summary = summary[:max_length * 3] + "..."
            
            return summary
        except Exception as e:
            logger.error(f"OpenAI API ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return self._enhanced_simple_summarize(content, max_length)

    def _enhanced_simple_summarize(self, content: str, max_length: int) -> str:
        """í–¥ìƒëœ ê·œì¹™ ê¸°ë°˜ ìš”ì•½ (OpenAI ì‹¤íŒ¨ì‹œ ëŒ€ì²´)"""
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?\n]+', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if not sentences:
            return content[:max_length]
        
        # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
        companies = self._extract_companies(content)
        numbers = self._extract_numbers(content)
        tech_keywords = self._extract_tech_keywords(content)
        
        # êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
        summary_parts = []
        
        # ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ
        important_sentences = []
        for sentence in sentences[:8]:
            score = self._sentence_importance_score(sentence)
            important_sentences.append((score, sentence))
        
        important_sentences.sort(reverse=True)
        
        # ğŸ“„ í•µì‹¬ ë‚´ìš© - ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ
        if important_sentences:
            core_sentences = [s[1] for s in important_sentences[:2]]
            core_content = ". ".join(core_sentences)[:200]
            summary_parts.append(f"ğŸ’¼ **ì‚°ì—… ë™í–¥ & ê¸°ìˆ  ì´í•´**\n{core_content}. ì´ëŠ” ë°˜ë„ì²´ ì‚°ì—…ì˜ ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ë³´ì—¬ì£¼ëŠ” ì¤‘ìš”í•œ ë°œì „ì…ë‹ˆë‹¤.\n\n")
        
        # ğŸ¢ ì£¼ìš” ê¸°ì—…/ê¸°ìˆ  - êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ í•¨ê»˜
        if companies or tech_keywords:
            entities = []
            if companies:
                entities.extend(companies[:2])
            if tech_keywords:
                entities.extend(tech_keywords[:2])
            
            entity_desc = ", ".join(entities[:4])
            if entity_desc:
                summary_parts.append(f"ğŸ­ **ì£¼ìš” ê¸°ì—… ë¶„ì„ & ì·¨ì—… ì‹œì¥**\n{entity_desc}ì™€ ê´€ë ¨ëœ ê¸°ì—…ë“¤ì´ ì£¼ë„í•˜ëŠ” ê¸°ìˆ  í˜ì‹ ìœ¼ë¡œ, ê´€ë ¨ ë¶„ì•¼ ì·¨ì—… ì‹œì¥ì— ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ì œê³µí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n\n")
        
        # ï¿½ êµ¬ì²´ì  ì„±ê³¼ ì§€í‘œ - ìˆ˜ì¹˜ì˜ ì˜ë¯¸ê¹Œì§€ í¬í•¨
        if numbers:
            number_descriptions = []
            for num in numbers[:3]:
                # í•´ë‹¹ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•œ ë¬¸ì¥ ì°¾ê¸°
                for sentence in sentences:
                    if num in sentence:
                        # ìˆ˜ì¹˜ ì£¼ë³€ ë§¥ë½ ì¶”ì¶œ
                        if '%' in num:
                            if any(keyword in sentence.lower() for keyword in ['ì „ë ¥', 'íš¨ìœ¨', 'ì†Œë¹„', 'ì ˆì•½', 'ì†Œëª¨']):
                                number_descriptions.append(f"ì „ë ¥ íš¨ìœ¨ì„± {num} ê°œì„ ")
                            elif any(keyword in sentence.lower() for keyword in ['ëŒ€ì—­í­', 'ì „ì†¡', 'ì²˜ë¦¬ ì†ë„', 'ë¹ ë¥¸']):
                                number_descriptions.append(f"ë°ì´í„° ì²˜ë¦¬ ì†ë„ {num} í–¥ìƒ")
                            elif any(keyword in sentence.lower() for keyword in ['ì„±ëŠ¥', 'í–¥ìƒ', 'ì†ë„']):
                                number_descriptions.append(f"ì„±ëŠ¥ {num} í–¥ìƒ")
                            elif any(keyword in sentence.lower() for keyword in ['ìš©ëŸ‰', 'ë©”ëª¨ë¦¬', 'ì €ì¥', 'í™•ì¥']):
                                number_descriptions.append(f"ìš©ëŸ‰ {num} í™•ì¥")
                            elif any(keyword in sentence.lower() for keyword in ['ì‹œì¥', 'ì ìœ ìœ¨', 'ë§¤ì¶œ']):
                                number_descriptions.append(f"ì‹œì¥ ì ìœ ìœ¨ {num} ì¦ê°€")
                            else:
                                number_descriptions.append(f"{num} ì„±ëŠ¥ ê°œì„ ")
                        elif 'nm' in num.lower():
                            number_descriptions.append(f"{num} ë¯¸ì„¸ ê³µì • ê¸°ìˆ ")
                        elif any(unit in num.upper() for unit in ['GB', 'TB', 'MB']):
                            if any(keyword in sentence.lower() for keyword in ['ë©”ëª¨ë¦¬', 'ì €ì¥', 'ìš©ëŸ‰']):
                                number_descriptions.append(f"{num} ë©”ëª¨ë¦¬ ìš©ëŸ‰")
                            elif any(keyword in sentence.lower() for keyword in ['ëŒ€ì—­í­', 'ì „ì†¡', 'ì†ë„']):
                                number_descriptions.append(f"{num} ë°ì´í„° ì „ì†¡ ì†ë„")
                            else:
                                number_descriptions.append(f"{num} ìš©ëŸ‰ ì‚¬ì–‘")
                        elif any(unit in num.upper() for unit in ['GHZ', 'MHZ']):
                            number_descriptions.append(f"{num} ë™ì‘ ì£¼íŒŒìˆ˜")
                        else:
                            # ê¸°íƒ€ ìˆ«ìì˜ ê²½ìš° ë¬¸ë§¥ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                            if any(keyword in sentence.lower() for keyword in ['ë…„', 'ì›”', 'ë¶„ê¸°', 'ì–‘ì‚°', 'ì¶œì‹œ', 'ì˜ˆì •']):
                                number_descriptions.append(f"{num} ì¶œì‹œ/ì–‘ì‚° ì¼ì •")
                            elif any(keyword in sentence.lower() for keyword in ['ì–µ', 'ì¡°', 'ë‹¬ëŸ¬', 'ì›', 'íˆ¬ì', 'ì˜ˆì‚°', 'ë¹„ìš©']):
                                number_descriptions.append(f"{num} íˆ¬ì ê·œëª¨")
                            elif any(keyword in sentence.lower() for keyword in ['ì¹©', 'ì½”ì–´', 'íŠ¸ëœì§€ìŠ¤í„°']):
                                number_descriptions.append(f"{num} í•˜ë“œì›¨ì–´ ì‚¬ì–‘")
                            else:
                                number_descriptions.append(f"{num} í•µì‹¬ ìˆ˜ì¹˜")
                        break
            
            if number_descriptions:
                descriptions_text = ", ".join(number_descriptions)
                summary_parts.append(f"ğŸ“ˆ **êµ¬ì²´ì  ì„±ê³¼ ì§€í‘œ**\n{descriptions_text} - ì´ëŸ¬í•œ êµ¬ì²´ì  ìˆ˜ì¹˜ë“¤ì€ ë©´ì ‘ì—ì„œ ê¸°ìˆ  íŠ¸ë Œë“œì™€ ì‹œì¥ ë™í–¥ ì´í•´ë„ë¥¼ ë³´ì—¬ì£¼ëŠ” ì¤‘ìš”í•œ ë°ì´í„°ì…ë‹ˆë‹¤.\n\n")
            else:
                # í´ë°±: ê¸°ì¡´ ë°©ì‹
                number_desc = ", ".join(numbers[:3])
                summary_parts.append(f"ğŸ“ˆ **êµ¬ì²´ì  ì„±ê³¼ ì§€í‘œ**\n{number_desc} ë“± í•µì‹¬ ìˆ˜ì¹˜ë“¤ì´ ë°œí‘œë˜ì–´ ë©´ì ‘ì—ì„œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n")
        
        # ğŸ¯ ì»¤ë¦¬ì–´ ì—°ê´€ì„± - êµ¬ì²´ì ì¸ ì—­ëŸ‰ê³¼ ì¤€ë¹„ ë°©í–¥ ì œì‹œ
        if len(important_sentences) > 2:
            insight_sentence = important_sentences[1][1] if len(important_sentences) > 1 else sentences[-1]
            
            # ê¸°ìˆ  í‚¤ì›Œë“œì— ë”°ë¥¸ í•„ìš” ì—­ëŸ‰ ì¶”ì²œ
            skill_recommendations = []
            if any(keyword in content.lower() for keyword in ['ai', 'ì¸ê³µì§€ëŠ¥', 'machine learning']):
                skill_recommendations.append("AI/ML ê´€ë ¨ í”„ë¡œê·¸ë˜ë°(Python, TensorFlow)")
            if any(keyword in content.lower() for keyword in ['ë°˜ë„ì²´', 'chip', 'ì¹©']):
                skill_recommendations.append("ë°˜ë„ì²´ ì„¤ê³„ ë„êµ¬(Cadence, Synopsys) ê²½í—˜")
            if any(keyword in content.lower() for keyword in ['í´ë¼ìš°ë“œ', 'cloud', 'ë°ì´í„°ì„¼í„°']):
                skill_recommendations.append("í´ë¼ìš°ë“œ í”Œë«í¼(AWS, Azure) í™œìš© ëŠ¥ë ¥")
                
            skills_text = ", ".join(skill_recommendations[:2]) if skill_recommendations else "ê´€ë ¨ ê¸°ìˆ  ìŠ¤íƒ"
            
            summary_parts.append(f"ğŸ¯ **ì»¤ë¦¬ì–´ ì—°ê´€ì„±**\n{insight_sentence[:100]}. ì´ëŸ¬í•œ ë³€í™”ë¡œ {skills_text} ë“±ì˜ ì—­ëŸ‰ì„ ê°–ì¶˜ ì¸ì¬ ìˆ˜ìš”ê°€ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ê´€ë ¨ ìê²©ì¦ ì·¨ë“ì´ë‚˜ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ìŒ“ëŠ” ê²ƒì´ ìœ ë¦¬í•©ë‹ˆë‹¤.\n\n")
        
        # ğŸ’¡ ë©´ì ‘ í™œìš© í¬ì¸íŠ¸ - êµ¬ì²´ì ì¸ ì˜ˆì‹œ ë‹µë³€ ì œê³µ
        if len(sentences) > 3:
            key_point = sentences[0][:80] if sentences else "ìµœì‹  ê¸°ìˆ  ë™í–¥"
            
            # ê¸°ì—…ëª… ì¶”ì¶œí•´ì„œ êµ¬ì²´ì ì¸ ì˜ˆì‹œ ë§Œë“¤ê¸°
            company_for_example = companies[0] if companies else "í•´ë‹¹ ê¸°ì—…"
            
            example_answer = f"ìµœê·¼ {company_for_example}ì˜ ë°œí‘œë¥¼ ë³´ë©´ {key_point.lower()}ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤"
            
            summary_parts.append(f"ğŸ’¡ **ë©´ì ‘ í™œìš© í¬ì¸íŠ¸**\n\"{example_answer}. ì´ëŸ¬í•œ ê¸°ìˆ  ë°œì „ì´ ì—…ê³„ ì „ë°˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•  ë•Œ, ì €ëŠ”...\"ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìµœì‹  ë™í–¥ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ì–´í•„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ìˆ ì˜ íŒŒê¸‰íš¨ê³¼ì™€ ë³¸ì¸ì˜ ê´€ë ¨ ê²½í—˜ì„ ì—°ê²°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n")
        
        final_summary = "\n".join(summary_parts)
        
        # ê¸¸ì´ ì œí•œ
        if len(final_summary) > max_length * 2.5:
            final_summary = final_summary[:int(max_length * 2.5)] + "..."
        
        return final_summary if final_summary else sentences[0][:max_length]
    
    def _extract_companies(self, text: str) -> List[str]:
        """ê¸°ì—…ëª… ì¶”ì¶œ"""
        companies = []
        company_patterns = [
            r'ì‚¼ì„±ì „ì|Samsung', r'SKí•˜ì´ë‹‰ìŠ¤|SK Hynix', r'TSMC|ëŒ€ë§Œë°˜ë„ì²´',
            r'ì¸í…”|Intel', r'AMD', r'NVIDIA|ì—”ë¹„ë””ì•„', r'í€„ì»´|Qualcomm',
            r'ì• í”Œ|Apple', r'ë§ˆì´í¬ë¡ |Micron', r'ë¸Œë¡œë“œì»´|Broadcom',
            r'ê¸€ë¡œë²ŒíŒŒìš´ë“œë¦¬|GlobalFoundries', r'ARM'
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                companies.extend(matches[:1])  # ì¤‘ë³µ ë°©ì§€
        
        return companies[:3]
    
    def _extract_numbers(self, text: str) -> List[str]:
        """ì¤‘ìš”í•œ ìˆ˜ì¹˜ ì¶”ì¶œ"""
        numbers = []
        number_patterns = [
            r'\d+%|\d+í¼ì„¼íŠ¸',  # í¼ì„¼íŠ¸
            r'\$\d+[MB]?|\d+ì–µ ë‹¬ëŸ¬|\d+ì¡° ë‹¬ëŸ¬',  # ë‹¬ëŸ¬ ê¸ˆì•¡
            r'\d+nm|\d+ë‚˜ë…¸',  # ê³µì • ê¸°ìˆ 
            r'\d+GB|\d+TB|\d+Gbps',  # ìš©ëŸ‰/ì†ë„
            r'\d+ë…„|\d+ì›”',  # ì‹œê°„
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, text)
            numbers.extend(matches[:2])
        
        return numbers[:3]
    
    def _extract_tech_keywords(self, text: str) -> List[str]:
        """ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ"""
        found_keywords = []
        tech_terms = [
            'AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì‹ ê²½ë§',
            'ì–‘ìì»´í“¨íŒ…', 'ì–‘ì', 'ì—£ì§€ì»´í“¨íŒ…', 'í´ë¼ìš°ë“œ',
            'ììœ¨ì£¼í–‰', 'IoT', '5G', '6G', 'ë¸”ë¡ì²´ì¸',
            'HBM', 'DDR5', 'GDDR6', 'SSD', 'CPU', 'GPU'
        ]
        
        text_lower = text.lower()
        for term in tech_terms:
            if term.lower() in text_lower and term not in found_keywords:
                found_keywords.append(term)
        
        return found_keywords[:3]

    def _sentence_importance_score(self, sentence: str) -> float:
        """ë¬¸ì¥ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        sentence_lower = sentence.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
        for keyword in self.high_priority_keywords:
            if keyword.lower() in sentence_lower:
                score += 2.0
        
        for keyword in self.medium_priority_keywords:
            if keyword.lower() in sentence_lower:
                score += 1.0
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©
        for keyword, weight in self.tech_keywords.items():
            if keyword.lower() in sentence_lower:
                score += weight
        
        # ìˆ«ìë‚˜ í¼ì„¼íŠ¸ê°€ ìˆìœ¼ë©´ ì¤‘ìš”ë„ ì¦ê°€
        if re.search(r'\d+%|\$\d+|\d+ì–µ|\d+ì¡°', sentence):
            score += 1.5
        
        return score

    def calculate_priority(self, article_data: Dict) -> float:
        """ê¸°ì‚¬ì˜ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚° (0-10ì )"""
        try:
            title = article_data.get('title', '')
            content = article_data.get('content', '')
            source = article_data.get('source', '')
            published_date = article_data.get('published_date', datetime.now())
            
            priority_score = 0.0
            
            # 1. ì œëª© ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 3ì )
            title_score = self._calculate_text_score(title) * 1.5
            priority_score += min(title_score, 3.0)
            
            # 2. ë‚´ìš© ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 3ì ) 
            content_score = self._calculate_text_score(content) * 0.8
            priority_score += min(content_score, 3.0)
            
            # 3. ì†ŒìŠ¤ ì‹ ë¢°ë„ ì ìˆ˜ (ìµœëŒ€ 3.5ì ) - ê¸°ì—… ë‰´ìŠ¤ë£¸ ë³´ë„ˆìŠ¤ í¬í•¨
            source_score = self._calculate_source_score(source)
            priority_score += source_score
            
            # 4. ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 2ì )
            time_score = self._calculate_time_score(published_date)
            priority_score += time_score
            
            # 5. ê¸°ì—… ë‰´ìŠ¤ë£¸ ê¸°ìˆ  ë‚´ìš© ì¶”ê°€ ë³´ë„ˆìŠ¤ (0-1.5ì )
            is_newsroom = any(keyword in source for keyword in ['Newsroom', 'newsroom', 'Press Release', 'press release'])
            if is_newsroom:
                priority_score += 0.5  # ë‰´ìŠ¤ë£¸ ê¸°ë³¸ ë³´ë„ˆìŠ¤
                
                # ë‰´ìŠ¤ë£¸ì˜ ë°˜ë„ì²´ ê¸°ìˆ  ë‚´ìš© ê²€ì‚¬
                combined_text = (title + ' ' + content).lower()
                tech_keywords_found = 0
                for keyword in self.tech_keywords.keys():
                    if keyword.lower() in combined_text:
                        tech_keywords_found += 1
                
                # ë°˜ë„ì²´ ê¸°ìˆ  í‚¤ì›Œë“œê°€ ë§ì„ìˆ˜ë¡ ì¶”ê°€ ë³´ë„ˆìŠ¤ (ìµœëŒ€ +1.0)
                if tech_keywords_found > 0:
                    tech_bonus = min(tech_keywords_found * 0.25, 1.0)
                    priority_score += tech_bonus
                    logger.debug(f"ê¸°ìˆ  í‚¤ì›Œë“œ {tech_keywords_found}ê°œ ë°œê²¬ -> +{tech_bonus:.2f} ë³´ë„ˆìŠ¤")
            
            # ìµœì¢… ì ìˆ˜ ì •ê·œí™” (0-10)
            final_score = min(priority_score, 10.0)
            
            logger.debug(f"ìš°ì„ ìˆœìœ„ ê³„ì‚°: {title[:30]}... -> ì ìˆ˜: {final_score:.2f}")
            return final_score
            
        except Exception as e:
            logger.error(f"ìš°ì„ ìˆœìœ„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return 5.0  # ê¸°ë³¸ê°’

    def _calculate_text_score(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        if not text:
            return 0.0
        
        text_lower = text.lower()
        score = 0.0
        
        # ê³ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ
        for keyword in self.high_priority_keywords:
            if keyword.lower() in text_lower:
                score += 2.0
        
        # ì¤‘ê°„ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ
        for keyword in self.medium_priority_keywords:
            if keyword.lower() in text_lower:
                score += 1.0
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        for keyword, weight in self.tech_keywords.items():
            if keyword.lower() in text_lower:
                score += weight
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ë³´ë„ˆìŠ¤
        patterns = [
            (r'\d+%|\d+í¼ì„¼íŠ¸', 1.0),  # í¼ì„¼íŠ¸
            (r'\$\d+|\d+ë‹¬ëŸ¬|\d+ì–µ|\d+ì¡°', 1.5),  # ê¸ˆì•¡
            (r'first|ìµœì´ˆ|first time|ì²˜ìŒ', 2.0),  # ìµœì´ˆ/ì²˜ìŒ
            (r'record|ê¸°ë¡|ìµœê³ |highest|lowest', 1.5),  # ê¸°ë¡
        ]
        
        for pattern, bonus in patterns:
            if re.search(pattern, text_lower):
                score += bonus
        
        return score

    def _calculate_source_score(self, source: str) -> float:
        """ë‰´ìŠ¤ ì†ŒìŠ¤ì˜ ì‹ ë¢°ë„ ì ìˆ˜ + ê¸°ì—… ë‰´ìŠ¤ë£¸ ë³´ë„ˆìŠ¤"""
        source_scores = {
            'Reuters': 2.0, 'ë¡œì´í„°': 2.0,
            'Bloomberg': 2.0, 'ë¸”ë£¸ë²„ê·¸': 2.0, 
            'Wall Street Journal': 1.8, 'WSJ': 1.8,
            'Financial Times': 1.8, 'FT': 1.8,
            'TechCrunch': 1.5, 'í…Œí¬í¬ëŸ°ì¹˜': 1.5,
            'The Verge': 1.3, 'Ars Technica': 1.5,
            'EE Times': 1.7, 'Semiconductor Engineering': 1.7,
            'ì „ìì‹ ë¬¸': 1.6, 'í•œêµ­ê²½ì œ': 1.4, 'ë§¤ì¼ê²½ì œ': 1.4,
            'AI Weekly': 1.2, 'TechNews': 1.0
        }
        
        score = 1.0  # ê¸°ë³¸ ì ìˆ˜
        
        for source_name, source_score in source_scores.items():
            if source_name.lower() in source.lower():
                score = source_score
                break
        
        # ê¸°ì—… ê³µì‹ ë‰´ìŠ¤ë£¸ì€ ìš°ì„ ìˆœìœ„ +1.5 (ë§¤ìš° ì‹ ë¢°ë„ ë†’ìŒ)
        if any(keyword in source for keyword in ['Newsroom', 'newsroom', 'Press Release', 'press release', 'Official', 'official']):
            score += 1.5
        
        return min(score, 3.5)  # ìµœëŒ€ 3.5

    def _calculate_time_score(self, published_date: datetime) -> float:
        """ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (ìµœì‹  ê¸°ì‚¬ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)"""
        if not published_date:
            return 1.0
        
        now = datetime.now()
        if published_date.tzinfo:
            now = now.replace(tzinfo=published_date.tzinfo)
        
        time_diff = now - published_date
        hours_ago = time_diff.total_seconds() / 3600
        
        # ì‹œê°„ì— ë”°ë¥¸ ì ìˆ˜ ê°ì†Œ
        if hours_ago <= 1:
            return 2.0      # 1ì‹œê°„ ì´ë‚´
        elif hours_ago <= 6:
            return 1.8      # 6ì‹œê°„ ì´ë‚´
        elif hours_ago <= 24:
            return 1.5      # 24ì‹œê°„ ì´ë‚´
        elif hours_ago <= 72:
            return 1.0      # 3ì¼ ì´ë‚´
        elif hours_ago <= 168:
            return 0.5      # 1ì£¼ ì´ë‚´
        else:
            return 0.2      # 1ì£¼ ì´ìƒ

    def analyze_trends(self, articles: List[Dict]) -> Dict:
        """ê¸°ì‚¬ë“¤ì˜ íŠ¸ë Œë“œ ë¶„ì„"""
        if not articles:
            return {}
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
        keyword_counts = {}
        for article in articles:
            text = (article.get('title', '') + ' ' + article.get('content', '')).lower()
            for keyword in self.tech_keywords.keys():
                if keyword.lower() in text:
                    keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # ìƒìœ„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
        top_trends = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ìˆ˜
        source_counts = {}
        for article in articles:
            source = article.get('source', 'Unknown')
            source_counts[source] = source_counts.get(source, 0) + 1
        
        return {
            'top_trends': top_trends,
            'source_distribution': source_counts,
            'total_articles': len(articles),
            'avg_priority': sum(article.get('priority_score', 0) for article in articles) / len(articles)
        }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    analyzer = NewsAnalyzer()
    
    # ìƒ˜í”Œ ê¸°ì‚¬ í…ŒìŠ¤íŠ¸
    sample_article = {
        'title': 'TSMC 3nm ê³µì • ê¸°ìˆ ë¡œ AI ì¹© ì„±ëŠ¥ 30% í–¥ìƒ',
        'content': 'TSMCê°€ ìµœì‹  3nm ê³µì • ê¸°ìˆ ì„ í†µí•´ AI ì¹©ì˜ ì„±ëŠ¥ì„ 30% í–¥ìƒì‹œì¼°ë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ì´ë²ˆ í˜ì‹ ì ì¸ ê¸°ìˆ ì€ ì „ë ¥ íš¨ìœ¨ì„±ë„ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤.',
        'source': 'TechNews',
        'published_date': datetime.now()
    }
    
    summary = analyzer.summarize_article(sample_article['content'])
    priority = analyzer.calculate_priority(sample_article)
    
    print(f"ìš”ì•½: {summary}")
    print(f"ìš°ì„ ìˆœìœ„ ì ìˆ˜: {priority:.2f}/10.0")